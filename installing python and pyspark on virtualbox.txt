SPARK AND PYTHON INSTALLATION ON LOCAL COMPUTER
This installation is for Windows machine
Download ubuntu


Download virtualbox from www.virtualbox.org
this virtualbox is a vitual machine on your local computer and here, we will install ubuntu

Download vitualbox (windows hosts)

-Open virtualbox
-Create virtual machine
    Name: Give your machine a name. i named mine 'myspark'
    Type : Linux
    Version : Ubuntu 64-bits
    * If the only option you saw is 32 bits and yout system's OS is 64 bits, restart your system,
      go to BIOS > System configuration >  restore securitysettings to defult > 
      enable virtual configuration > save*
    Choose memory size usually halve of your RAM size. if you have a 4GB RAM, assign 2GB
    Hard Disk (default is 8GB)
    Select create a virtual hard disk now
    Hard Disk file type
      Select VDI
      Storage on physical hard disk
      Fixed size
      File location and size (20GB)
    Create
After creating, 
     right click on settings
     System > Processor > max green
     Click on Display
     Video memory >1/2 way > ok
     Storage> controlled IDE> Select Empty> on the right, under attributes> click the disk image and select ubuntu that you just downloaded.
Start up disk >Select ubuntu you downloaded
Ubuntu will be installed on your virtual machine
Download updates while installing ubuntu
continue
Erase disk and install ubuntu (dont worry, this will not erase your disk)
Install now
Continue
Select your time zone
Continue
Choose your preferred language
Fill up 'who are you'
log in

If locked, press esc and login with your password
Ubuntu installed comes with python on it
go to terminal and type python
To install jupyter, pip3 install jupyter(if this does not work use pip snap install jupyter)
IF pip3 is not installed install it with the code below
sudo apt install python3 -pip
if this does not work, do the following
sudo add-apt-repository universe
sudo apt update
sudo apt install python3-pip 
pip3 install jupyter

If there persist error message 'unable to lock directory /var/lib/dpkg, do the following:
- First find and kill all apt-get or apt processes
- list the apt processes with grep command stated below:
     $ ps -A | grep apt
- Kill the error that has a particular process ID, for example
     sudo kill -9 2678
- Delete the lock file
     sudo rm /var/lib/dpkg/lock
- Force packages to reconfigure like below:
     sudo dpkg --configure -a
- Update your package list
     sudo apt-get update

Now carry on with the installations




To install spark, we install jave first
sudo apt-get update
sudo apt-get install default-jre

Jave installed
to confirm if it worked, check the version of jave installed

type java -version in the terminal

Next install scala
sudo apt-get install scala

Scala installed, check the version

scala -version

Next install py4j. This connect python with java and scala

pip3 install py4j

Now install apache-spark
First go to your browser and search apache spark
download with hadoop select save file. it will download the latest version that look like this
spark-2.1.0-bin-hadoop 2.7.dz (or the latest version which could be 3.0.0
Move it from download to home folder
sudo tar -zxvf spark-2.1.0-bin-hadoop 2.7.tgz (you can press tab to autocomplete the spark version you downloaded) to unzip the spark
You will be prompted for your password
The file will be unzipped
Now let us tell python where to find Spark
ADD PATH
export SPARK_HOME='home/ubuntu/spark-2.1.0-bin-hadoop 2.7'
export PATH=$SPARK_HOME:$PATH
export PYTHONPATH = $SPARK_HOME/python:$PYTHONPATH
export PYSPARK_DRIVER_PYTHON="jupyter"
export PYSPARK_DRIVER_PYTHON_OPTS="notebook"
export PYSPARK_PYTHON=python3

if you want to access python locked folder,for instance our spark folder
chmod 777 spark-2.1.0-bin-hadoop 2.7 (you can add sudo in front)
then cd spark
cd python 
python3
Now try import pyspark, it should work
To access jupyter notebook inside python folder
sudo chmod 777 python
cd python
sudo chmod 777 pyspark
jupyter notebook
try import pyspark inside jupyer notebook
In order to be able to use pyspark in any directory
pip3 install findspark

to find the folder containing pyspark
cd into spark-2.1.0-bin-hadoop 2.7
pwd to show the path, copy this path
cd to return back to home directory jupyter notebook
inside jupyter notebook
import findspark
findspark.init('the path you copied')
import pyspark (it will work)


TO SHARE FILES AND FOLDER BETWEEN HE VIRTUAL MACHINE AND YOUR LOCAL COMPUTER

Create a folder on local computer, and name it
right click > permission > share > Everyone > read/write > save
Open virtualbox
click settings > Shared folders > others > select the folder you created >auto mount >save
go to devices > install guest additions (if this is is not visible, download the VBoxGuestAdditions.iso for the version os spark you have and mount it under Storage> Controller:SATA
Go to terminal
sudo apt install virtualbox-guest-utils virtualbox-guest-dkms
Install VBox guest additions
sudo adduser 'folder_name' vboxsf
sudo reboot
Now open the folder

